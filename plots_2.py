import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
import seaborn as sns
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
from itertools import cycle
from sklearn.model_selection import validation_curve, learning_curve

def plot_per_class_metrics(y_test, y_pred, class_names, model_name):
    
    output_dir = os.path.join("Plots", model_name)
    os.makedirs(output_dir, exist_ok=True)

    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, labels=range(len(class_names)), zero_division=0, average=None)

    plt.figure(figsize=(12, 6))
    x = np.arange(len(class_names))
    width = 0.25

    plt.bar(x - width, precision, width, label='Precision')
    plt.bar(x, recall, width, label='Recall')
    plt.bar(x + width, f1_score, width, label='F1-score')

    plt.xticks(x, class_names, rotation=45)
    plt.ylabel("Score")
    plt.title(f"Per-Class Metrics ({model_name})")
    plt.legend()
    plt.tight_layout()

    plt.savefig(os.path.join(output_dir, f"per_class_metrics_{model_name.lower()}.png"))
    plt.close()
    print(f"Per-class metrics plot saved for model {model_name} in {output_dir}")

def plot_confusion_matrix(y_test, y_pred, class_names, model_name):
    output_dir = os.path.join("Plots", model_name)
    os.makedirs(output_dir, exist_ok=True)

    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt="d",
                xticklabels=class_names,
                yticklabels=class_names,
                cmap="Blues")
    
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(f"Confusion Matrix ({model_name})")
    plt.tight_layout()

    plt.savefig(os.path.join(output_dir, f"confusion_matrix_{model_name.lower()}.png"))
    plt.close()
    print(f"Confusion matrix saved for model {model_name} in {output_dir}")

def plot_roc_curve(y_test, y_pred, model_name, class_names):
    # NOTA: y_pred aqu√≠ deben ser PROBABILIDADES (predict_proba), no etiquetas
    output_dir = os.path.join("Plots", model_name)
    os.makedirs(output_dir, exist_ok=True)

    y_test_binarized = label_binarize(y_test, classes=range(len(class_names)))
    n_classes = y_test_binarized.shape[1]
    
    plt.figure(figsize=(10, 8))
    colors = cycle(['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan'])
    
    # Itera sobre cada clase calculando su curva individual contra el resto
    for i, color in zip(range(n_classes), colors):
        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, color=color, lw=2,
                 label=f'ROC curve of class {class_names[i]} (AUC = {roc_auc:0.2f})')
            
    plt.plot([0, 1], [0, 1], 'k--', lw=2) # Linia diagonal == Azar
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve ({model_name})')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    plt.tight_layout()

    plt.savefig(os.path.join(output_dir, f"roc_curve_{model_name.lower()}.png"))
    plt.close()
    print(f"ROC curve saved for model {model_name} in {output_dir}") 

def plot_precision_recall_curve(y_test, y_pred, model_name, class_names):
    # NOTA: y_pred aqu√≠ deben ser PROBABILIDADES (predict_proba)
    output_dir = os.path.join("Plots", model_name)
    os.makedirs(output_dir, exist_ok=True)

    y_test_binarized = label_binarize(y_test, classes=range(len(class_names)))
    n_classes = y_test_binarized.shape[1]

    plt.figure(figsize=(10, 8))
    colors = cycle(['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan'])

    # Calcula Precisi√≥n vs Recall para cada clase.
    for i, color in zip(range(n_classes), colors):
        precision, recall, _ = precision_recall_curve(y_test_binarized[:, i], y_pred[:, i])
        avg_precision = average_precision_score(y_test_binarized[:, i], y_pred[:, i])            
        plt.plot(recall, precision, color=color, lw=2,
                 label=f'Precision-Recall curve of class {class_names[i]} (AP = {avg_precision:0.2f})')
        
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve ({model_name})')
    plt.legend(loc="best")
    plt.grid(alpha=0.3)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.tight_layout()

    plt.savefig(os.path.join(output_dir, f"precision_recall_curve_{model_name.lower()}.png"))
    plt.close()
    print(f"Precision-Recall curve saved for model {model_name} in {output_dir}")


def plot_general_roc_curve(y_test, y_pred, model_name, class_names):
    """
    Genera una √∫nica curva ROC (Micro-average) que representa el rendimiento 
    global del modelo. Ideal para comparar entre diferentes modelos.
    """
    output_dir = os.path.join("Plots", model_name)
    os.makedirs(output_dir, exist_ok=True)

    # Binarizar para convertir etiquetas en formato one-hot
    y_test_binarized = label_binarize(y_test, classes=range(len(class_names)))
    
    # --- C√ÅLCULO MICRO-AVERAGE (GLOBAL) ---
    # .ravel() aplana los arrays para tratarlos como un √∫nico conjunto binario gigante
    fpr, tpr, _ = roc_curve(y_test_binarized.ravel(), y_pred.ravel())
    roc_auc = auc(fpr, tpr)

    # Plot
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'General ROC (Micro-avg) (AUC = {roc_auc:0.3f})')
    
    plt.plot([0, 1], [0, 1], 'k--', lw=2) # L√≠nea de azar
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'General ROC Curve - {model_name}')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    plt.tight_layout()

    filename = os.path.join(output_dir, f"general_roc_{model_name.lower()}.png")
    plt.savefig(filename)
    plt.close()
    print(f"General ROC saved: {filename}")

def plot_general_pr_curve(y_test, y_pred, model_name, class_names):
    """
    Genera una √∫nica curva PR (Micro-average) para evaluaci√≥n global.
    """
    output_dir = os.path.join("Plots", model_name)
    os.makedirs(output_dir, exist_ok=True)

    y_test_binarized = label_binarize(y_test, classes=range(len(class_names)))
    
    precision, recall, _ = precision_recall_curve(y_test_binarized.ravel(), y_pred.ravel())
    avg_precision = average_precision_score(y_test_binarized, y_pred, average="micro")

    # Plot
    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='blue', lw=2,
             label=f'General PR (Micro-avg) (AP = {avg_precision:0.3f})')
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'General Precision-Recall Curve - {model_name}')
    plt.legend(loc="best")
    plt.grid(alpha=0.3)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.tight_layout()

    filename = os.path.join(output_dir, f"general_pr_{model_name.lower()}.png")
    plt.savefig(filename)
    plt.close()
    print(f"General PR saved: {filename}")

def plot_class_distribution(y_train, y_test, label_encoder, filename='class_distribution_bar_chart.png'):
    """
    Genera i desa un gr√†fic de barres mostrant la distribuci√≥ de classes (g√®neres) 
    en tot el dataset (Train + Test).
    """
    
    # 1. Unim les etiquetes (codificades)
    y_total_encoded = np.concatenate([y_train, y_test])

    # 2. Descodifiquem a noms de classe
    y_total_names = label_encoder.inverse_transform(y_total_encoded)

    # 3. Contem la freq√º√®ncia
    y_labels = pd.Series(y_total_names)
    class_counts = y_labels.value_counts().sort_index()
    
    output_dir = "Plots"
    os.makedirs(output_dir, exist_ok=True)

    # 4. Crear el gr√†fic de barres
    plt.figure(figsize=(12, 6))
    class_counts.plot(kind='bar', color='darkgreen')

    plt.title('Distribuci√≥ de G√®neres Musicals (Total Dataset)', fontsize=14)
    plt.xlabel('G√®nere Musical', fontsize=12)
    plt.ylabel('N√∫mero de Mostres', fontsize=12)

    plt.xticks(rotation=45, ha='right')
    plt.grid(axis='y', linestyle='--', alpha=0.6)

    # Posa els valors sobre les barres
    for index, value in enumerate(class_counts):
        plt.text(index, value, f'{value}', ha='center', va='bottom', fontsize=10)

    plt.tight_layout()

    # Guardar
    output_file_path = os.path.join(output_dir, filename)
    plt.savefig(output_file_path)
    plt.close()
    print(f"‚úÖ Gr√†fic de distribuci√≥ de classes desat a '{output_file_path}'")


def plot_feature_importances(model, feature_names, model_name, top_n=20):
    """
    Dibuixa les top N variables m√©s importants d'un model basat en arbres (RF, XGBoost).
    
    Args:
        model: Model entrenat (ha de tenir l'atribut .feature_importances_).
        feature_names (list): Noms de les columnes (features) originals.
        model_name (str): Nom del model per al t√≠tol i fitxer.
        top_n (int): Nombre de variables a mostrar.
    """
    
    output_dir = os.path.join("Plots", model_name)
    os.makedirs(output_dir, exist_ok=True)
    
    importances = pd.Series(model.feature_importances_, index=feature_names)
    importances = importances.sort_values(ascending=False).head(top_n)

    plt.figure(figsize=(12, 6))
    sns.barplot(x=importances.values, y=importances.index, palette="rocket")
    
    plt.title(f"Top {top_n} Variables M√©s Importants ({model_name})", fontsize=16)
    plt.xlabel("Import√†ncia (Score)")
    plt.ylabel("Variables")
    plt.tight_layout()

    plot_path = os.path.join(output_dir, f"feature_importances_{model_name.lower()}.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()
    print(f"‚úÖ Gr√†fic d'import√†ncia de variables desat a: {plot_path}")

def plot_comparative_roc(y_test, probabilities_dict, model_names, class_names, data_type):
    """
    Genera una √∫nica corba ROC (Micro-average) per a tots els models comparats.

    Args:
        y_test (np.array): Etiquetes reals codificades (num√®riques).
        probabilities_dict (dict): Diccionari {nom_model: y_prob_test}.
        model_names (list): Llista de noms dels models.
        class_names (np.array): Noms de les classes.
        data_type (str): Tipus de dades ('3s' o '30s').
    """
    
    output_dir = os.path.join("Plots", "Comparativa_Global")
    os.makedirs(output_dir, exist_ok=True)

    # Binaritzar per convertir etiquetes en format one-hot (necessari per a micro-average)
    y_test_binarized = label_binarize(y_test, classes=range(len(class_names)))
    
    plt.figure(figsize=(10, 8))
    
    # 1. Iterar sobre cada model i dibuixar la seva corba ROC
    for name in model_names:
        y_prob = probabilities_dict[name]
        
        # C√†lcul Micro-average ROC (Global)
        fpr, tpr, _ = roc_curve(y_test_binarized.ravel(), y_prob.ravel())
        roc_auc = auc(fpr, tpr)
        
        plt.plot(fpr, tpr, lw=2, 
                 label=f'{name} (AUC = {roc_auc:0.3f})')
        
    # 2. Afegir la l√≠nia de l'atzar
    plt.plot([0, 1], [0, 1], 'k--', lw=2) # L√≠nia diagonal == Azar

    # 3. Configuraci√≥ del gr√†fic
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (FPR)')
    plt.ylabel('True Positive Rate (TPR)')
    plt.title(f'Comparativa ROC (Micro-Avg) - Dataset {data_type.upper()}')
    plt.legend(loc="lower right", fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()

    filename = os.path.join(output_dir, f"comparativa_roc_{data_type.lower()}.png")
    plt.savefig(filename)
    plt.close()
    print(f"‚úÖ Gr√†fic comparatiu ROC desat a: {filename}")

def plot_single_validation_curve(estimator, X, y, param_name, param_range, title, xlabel, SAVE_DIR):
    print(f"   ‚öôÔ∏è  Generando curva de validaci√≥n para: {param_name}...")
    
    # Calculamos la precisi√≥n en Train y en Test (Cross-Validation)
    train_scores, test_scores = validation_curve(
        estimator, X, y, 
        param_name=param_name, 
        param_range=param_range,
        cv=3, 
        scoring="accuracy", 
        n_jobs=-1
    )

    # Medias y desviaciones est√°ndar
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel("Accuracy")
    plt.ylim(0.0, 1.1)
    
    # Dibujamos Train (Naranja)
    plt.plot(param_range, train_mean, label="Training Score", color="darkorange", lw=2)
    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color="darkorange")
    
    # Dibujamos Test/Validaci√≥n (Azul) - ESTA ES LA IMPORTANTE
    plt.plot(param_range, test_mean, label="Cross-Validation Score", color="navy", lw=2, marker='o')
    plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.1, color="navy")
    
    plt.legend(loc="best")
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    
    filename = f"{SAVE_DIR}/VC_{param_name}.png"
    plt.savefig(filename)
    plt.close()
    print(f"   ‚úÖ Guardado: {filename}")

def plot_final_learning_curve(estimator, X, y, title, SAVE_DIR):
    print("   üìà Generando Curva de Aprendizaje (Learning Curve)...")
    
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y,
        cv=3,
        n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 5), # 10%, 32%, 55%, 77%, 100% de datos
        scoring="accuracy"
    )

    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.title(title)
    plt.xlabel("N√∫mero de muestras de entrenamiento")
    plt.ylabel("Accuracy")
    plt.grid(True, linestyle='--')
    
    # Bandas de error
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="g")
    
    # L√≠neas
    plt.plot(train_sizes, train_mean, 'o-', color="r", label="Training Score")
    plt.plot(train_sizes, test_mean, 'o-', color="g", label="Cross-Validation Score")

    plt.legend(loc="best")
    plt.tight_layout()
    plt.savefig(f"{SAVE_DIR}/Learning_Curve_Final.png")
    plt.close()
    print(f"   ‚úÖ Guardado: {SAVE_DIR}/Learning_Curve_Final.png")